# log
log_interval_detail: 2000

# data related, check `utils/dataflow.py`
dataset: imagenet1k_lmdb  # different dataset, could be one of ['imagenet1k', 'imagenet1k_lmdb']
data_transforms: imagenet1k_mnas_bicubic  # preprocessing strategy
data_loader: imagenet1k_basic  # 'imagenet1k_basic' only
data_loader_workers: 62  # number of total workers

# basic info
image_size: 224
topk: [1, 5]  # log top-k acc
num_epochs: 350  # number of total epochs

# optimizer, check `utils/optim.py`
optimizer: rmsprop
momentum: 0.9
alpha: 0.9
epsilon: 0.001
eps_inside_sqrt: True
weight_decay: 1.0e-5
weight_decay_method: mnas

# lr scheduler, check `utils/optim.py`
base_lr: 0.016
base_total_batch: 256
per_gpu_batch_size: 64
distill: False
lr_scheduler: exp_decaying
lr_stepwise: False
exp_decaying_lr_gamma: 0.97
exp_decay_epoch_interval: 2.4

# label smoothing, check `utils/optim.py`
label_smoothing: 0.1

# exponential moving average for model var, check `utils/optim.py`
moving_average_decay: 0.9999
moving_average_decay_adjust: True
moving_average_decay_base_batch: 4096

# model profiling
profiling: [gpu]  # on GPU only

# log
log_interval: 100  # log every xxx iterations

# pretrain, resume, test_only
pretrained: ''
resume: ''
test_only: False

# other
random_seed: 1995
reset_parameters: True
reset_param_method: mnas


# must override
use_distributed: True  # whether to use distributed training
allreduce_bn: False  # whether to sync BN'statistics every iteration

model: models.hrnet
model_kwparams: {
  active_fn: 'nn.ReLU',
  num_classes: 1000,
  width_mult: 1.0,
  round_nearest: 2,
  input_stride: 4,
  bn_momentum: 0.01,
  bn_epsilon: 1.0e-3,

  input_channel: [24, 24],
  last_channel: 1600,
  expand_ratio: 4,
  kernel_sizes: [3, 5, 7],
  inverted_residual_setting: [
    [1, [1], [24]],
    [2, [2, 2], [18, 36]],
    [3, [2, 2, 3], [18, 36, 72]],
    [4, [2, 2, 3, 4], [18, 36, 72, 144]],
    [4, [2, 2, 3, 4], [18, 36, 72, 144]]
  ],
  head_channels: [36, 72, 144, 320],
  task: classification
}
net_params: ''
prune_params: {
  method: network_slimming,  # use ~ to represent None
  bn_prune_filter: expansion_only,
  rho: 1.8e-4,  # weight for l1 regularization loss
  epoch_free: 0,  # [0, `epoch_free`] without penalty
  epoch_warmup: 25,  # [`epoch_free`, `epoch_warmup`] gradually increase `rho`
  scheduler: linear,  # how to do warmup
  stepwise: True,  # warmup stepwise
  logging_verbose: False,
  use_transformer: False
}

# shrink model if xxx FLOPS is pruned, default never shrink for back compatibility
model_shrink_threshold: 1.0e-3
model_shrink_delta_flops: 1.0e+6
bn_calibration: True
bn_calibration_steps: 10
bn_calibration_per_gpu_batch_size: 256

# override part of model params
'model_kwparams.batch_norm_momentum': 0.01
'model_kwparams.batch_norm_epsilon': 1.0e-3

use_hdfs: False
eval_interval: 5

dataset_dir: ./data/imagenet_lmdb  # dataset root
log_dir: output/cls_imagenet
